#Survival Prediction with Random Forest

This project implements a Random Forest Classifier on the Kaggle Titanic dataset to improve prediction accuracy and interpretability.

Project Overview

Preprocessed and encoded the dataset for model compatibility

Split data into training and testing sets for evaluation

Trained a RandomForestClassifier with 100 estimators

Achieved strong accuracy on both training and testing data

Visualized feature importance using Matplotlib

Extracted and visualized individual Decision Trees for interpretability

Why Random Forest

Random Forests extend Decision Trees by combining multiple trees to improve generalization and reduce overfitting. This makes them one of the most effective supervised learning algorithms for classification tasks.

Key Steps

Dataset preprocessing and feature encoding

Train-test split for robust evaluation

Model training and prediction using RandomForestClassifier

Accuracy scoring for validation

Feature importance visualization

Individual tree extraction and visualization

Tech Stack

Python

Pandas

NumPy

Scikit-learn

Matplotlib

Tags

Machine Learning, Data Science, Python, Random Forest, Kaggle, Titanic Dataset, Scikit-learn, Supervised Learning, Feature Engineering, Model Evaluation
